---
layout: post
title: "Categorial feature 처리" 
categories:
  - Data
tags:
  - Blog
use_math: true
comments: true
---

# Multi Classification
## 변이 유전자 탐색 및 분류

머신러닝과 딥러닝 이론을 어느정도 공부하고 진짜 문제와 이론을 접목시켜보기 위해 캐글 같은 프로젝트에 도전해보고 있다.

현재는 캐글에서 [변이 유전자 탐색](https://www.kaggle.com/c/msk-redefining-cancer-treatment/data) 프로젝트에 도전하고 있는데, 결국 1~9까지의 Mulit classification 유형이다.

문제는 feature가 Gene, Variation(변이), Text로 전부 categorical이라는 것. Text feature의 경우 길이도 엄청 길어서, NPL에 대한 이해도 필요할 것 같다.

그건 나중에 보기로 하고 우선 Gene과 Variation을 처리하자면, Gene의 nunique는 264, Variation은 2996으로 One-hot-Encoding으로 처리하면 컬럼이 너무 많아진다.

## Evaluation Metrics

또 한 가지 주의점은 Evaluation Metrics가 [Log Loss](http://wiki.fast.ai/index.php/Log_Loss)란 것. 이진 분류인 경우 수직은 다음과 같은데
$$-(ylog(p)+(1-y)log(1-p))$$

class 뿐 아니라 확률까지 고려하는 방식이다. 클래스를 정확히 분류했더라도, 다른(틀린) 클래스에도 확률을 부여했다면(그래서 정답 확률이 낮아졌다면) 거기에 패널티를 부여하는 방식으로 작동한다. 범위는 0~무한대이고 당연히 낮을수록 좋은데, good bad 기준이 명확하지 않다. 따라서 랜덤모델(worst case)을 만들고 거기에서 시작하는 게 좋은 방법.

Metric이 Log loss라 모델이 도출하는 건 각 클래스의 확률이어야 한다.

## Handling categorical features
### One Hot Encoding

Categorical feature(특히 문자형)는 보통 One-Hot-Encoding을 하는데 이것도 여러 방식이 있다.

```python
# pandas
pd.get_dummies(df['Gene'])

```





