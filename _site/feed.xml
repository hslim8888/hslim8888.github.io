<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/blog/" rel="alternate" type="text/html" /><updated>2021-08-16T17:39:50+09:00</updated><id>http://localhost:4000/blog/feed.xml</id><title type="html">Date with Data</title><subtitle>데이터 사이언스 </subtitle><author><name>hslim8888</name><email>hslim8888@gmail.com</email></author><entry><title type="html">One Hot Encoding</title><link href="http://localhost:4000/blog/classification/One-Hot-Encoding/" rel="alternate" type="text/html" title="One Hot Encoding" /><published>2020-05-20T00:00:00+09:00</published><updated>2020-05-20T00:00:00+09:00</updated><id>http://localhost:4000/blog/classification/One-Hot-Encoding</id><content type="html" xml:base="http://localhost:4000/blog/classification/One-Hot-Encoding/">&lt;h1 id=&quot;multi-classification&quot;&gt;Multi Classification&lt;/h1&gt;
&lt;h2 id=&quot;변이-유전자-탐색-및-분류&quot;&gt;변이 유전자 탐색 및 분류&lt;/h2&gt;

&lt;p&gt;머신러닝과 딥러닝 이론을 어느정도 공부하고 진짜 문제와 이론을 접목시켜보기 위해 캐글 같은 프로젝트에 도전해보고 있다.&lt;/p&gt;

&lt;p&gt;현재는 캐글에서 &lt;a href=&quot;https://www.kaggle.com/c/msk-redefining-cancer-treatment/data&quot;&gt;변이 유전자 탐색&lt;/a&gt; 프로젝트에 도전하고 있는데, 결국 1~9까지의 Mulit classification 유형이다.&lt;/p&gt;

&lt;p&gt;문제는 feature가 Gene, Variation(변이), Text로 전부 categorical이라는 것. Text feature의 경우 길이도 엄청 길어서, NPL에 대한 이해도 필요할 것 같다.&lt;/p&gt;

&lt;p&gt;그건 나중에 보기로 하고 우선 Gene과 Variation을 처리하자면, Gene의 nunique는 264, Variation은 2996으로 One-hot-Encoding으로 처리하면 컬럼이 너무 많아진다.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-metrics&quot;&gt;Evaluation Metrics&lt;/h2&gt;

&lt;p&gt;또 한 가지 주의점은 Evaluation Metrics가 &lt;a href=&quot;http://wiki.fast.ai/index.php/Log_Loss&quot;&gt;Log Loss&lt;/a&gt;란 것. 이진 분류인 경우 수직은 다음과 같은데&lt;/p&gt;

\[-(ylog(p)+(1-y)log(1-p))\]

&lt;p&gt;class 뿐 아니라 확률까지 고려하는 방식이다. 클래스를 정확히 분류했더라도, 다른(틀린) 클래스에도 확률을 부여했다면(그래서 정답 확률이 낮아졌다면) 거기에 패널티를 부여하는 방식으로 작동한다. 범위는 0~무한대이고 당연히 낮을수록 좋은데, good bad 기준이 명확하지 않다. 따라서 랜덤모델(worst case)을 만들고 거기에서 시작하는 게 좋은 방법.&lt;/p&gt;

&lt;p&gt;Metric이 Log loss라 모델이 도출하는 건 각 클래스의 확률이어야 한다.&lt;/p&gt;

&lt;h2 id=&quot;handling-categorical-features&quot;&gt;Handling categorical features&lt;/h2&gt;
&lt;h3 id=&quot;one-hot-encoding&quot;&gt;One Hot Encoding&lt;/h3&gt;

&lt;p&gt;범주형 데이터(특히 문자형)는 보통 One-Hot-Encoding을 하는데 이것도 여러 방식이 있다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 1) pandas
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_dummies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Gene&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 2) OneHotEncoder
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OneHotEncoder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OneHotEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Gene&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3) CountVectorizer
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CountVectorizer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Gene&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pandas를 이용한 1번이 가장 편하긴한데, 아래와 같이 DataFrame을 생성해 메모리 소모가 있다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;n&quot;&gt;ABL1&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;ACVR1&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;AGO2&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;AKT1&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;AKT2&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;AKT3&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;ALK&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;APC&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;AR&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;ARAF&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;TSC1&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;TSC2&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;U2AF1&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;VEGFA&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;VHL&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;WHSC1&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;WHSC1L1&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;XPO1&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;XRCC2&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;YAP1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1019&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;676&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;243&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;901&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;	&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;반면, 2),3)은 generator(맞나?)로 필요할 때 사용하기 때문에 메모리 사용은 덜하다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2124&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x235&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparse&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;&amp;lt;class &apos;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&amp;gt;&apos;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2124&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elements&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Compressed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sparse&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Row&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이 프로젝트를 공부하면서 3번 방법을 처음 알게 되었는데, array화시키고, reshape도 신경써줘야하는 2번에 비해 편하다.&lt;/p&gt;

&lt;h2 id=&quot;response-coding&quot;&gt;Response Coding&lt;/h2&gt;

&lt;p&gt;One hot encoding의 단점은 category에 nunique가 많을 경우 생성되는 컬럼이 너무 많다는 것. 해당 프로젝트의 Metric이 각 클래스의 확률을 고려하는 Log Loss란 점을 고려해 Response Coding을 이용하는 게 좋아보이는데 Response Coding에 대해선 다음 글에 다루겠다.&lt;/p&gt;</content><author><name>hslim8888</name><email>hslim8888@gmail.com</email></author><category term="classification" /><category term="Blog, classification, categorical, 범주형, NLP" /><summary type="html">Multi Classification 변이 유전자 탐색 및 분류</summary></entry><entry><title type="html">Response Coding</title><link href="http://localhost:4000/blog/classification/responce-coding/" rel="alternate" type="text/html" title="Response Coding" /><published>2020-05-20T00:00:00+09:00</published><updated>2020-05-20T00:00:00+09:00</updated><id>http://localhost:4000/blog/classification/responce-coding</id><content type="html" xml:base="http://localhost:4000/blog/classification/responce-coding/">&lt;h1 id=&quot;범주형-데이터&quot;&gt;범주형 데이터&lt;/h1&gt;
&lt;h2 id=&quot;response-coding&quot;&gt;Response Coding&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://hslim8888.github.io/classification/One-Hot-Encoding/&quot;&gt;One-hot-encoding에 이어서&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Response Coding은 조건부 확률을 일컫는 것이나 다름없다.&lt;a href=&quot;https://medium.com/@thewingedwolf.winterfell/response-coding-for-categorical-data-7bb8916c6dc1&quot;&gt;참조&lt;/a&gt;
수식은 다음과 같은데&lt;/p&gt;

\[P(class=X | category=A) = P(category=A ∩ class=X) / P(category=A)\]

&lt;p&gt;category에 따른 class의 확률을 구하는 것이라, Category의 개수만큼 차원(feature)이 생기는 원핫인코딩과는 달리 Response Coding은 &lt;strong&gt;&lt;em&gt;class의 개수만큼&lt;/em&gt;&lt;/strong&gt; 차원(feature)이 늘어난다.&lt;/p&gt;

&lt;p&gt;또한 클래스당 확률을 고려하는 거라 모델의 측정 지표인 Log loss와도 궁합이 맞는 것 같다.&lt;/p&gt;

&lt;p&gt;찾아보니 One hot encoding은 Logistic Regression, SVM에 쓰면 좋고, Response Coding은 나이브 베이즈 모델, KNN, 랜덤 포레스트 모델에 쓰면 좋다고 하는데 실제 그런지는 기회가 되면 테스트 해봐야겠다.&lt;/p&gt;

&lt;h2 id=&quot;laplace-smoothing-additive-smoothing&quot;&gt;Laplace Smoothing (Additive Smoothing)&lt;/h2&gt;

&lt;p&gt;머신러닝에 조건부 확률을 쓸 때의 문제는 train, test set을 나눈다는 점이다.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;class가 0,1,2 세 개이고 train의 카테고리가 A, B, C, D, E가 있을 때, P(class=1&lt;/td&gt;
      &lt;td&gt;category=A)를 구할 수 있다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;하지만 Test set에선 카테고리가 A,B,C,D,E,F로 train set에 없던 F가 더 있을 수 있으며, 이 경우 훈련 모델에선 P(F)=0라 위 수식에선 분모가 0이 되거나&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/1386ec6778f1816c3fa6e9de68f89cee2e938066&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule_(probability)&quot;&gt;Chain Rule&lt;/a&gt; 에선 값이 무조건 0이 되어버린다.&lt;/p&gt;

&lt;p&gt;이런 문제를 간단히 해소하는 것이 바로 Laplace Smoothing(라플라스 평활)이다. &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_smoothing&quot;&gt;위키&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;\(p_i = x_i/N\) 에서 \(p_i = (x_i+alpha)/(N+alpha*K)\) 로 바꿔준 건데, K는 class의 개수이다.&lt;/p&gt;

&lt;p&gt;위의 예에서 N=100, alpha=1이라 했을 때,&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;원래라면 $$P(F&lt;/td&gt;
      &lt;td&gt;1)=0/100\(이지만, 라플라스 평활을 이용하면\)P(F&lt;/td&gt;
      &lt;td&gt;1)=(0+1)/(1000+3)$$ 으로 값이 0이 아니게 된다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;따라서 머신러닝 문제에서 조건부 확률을 이용할 땐 무조건 Laplace Smoothing을 고려해야한다고 생각하면 될 듯.&lt;/p&gt;</content><author><name>hslim8888</name><email>hslim8888@gmail.com</email></author><category term="classification" /><category term="Blog" /><summary type="html">범주형 데이터 Response Coding One-hot-encoding에 이어서</summary></entry></feed>