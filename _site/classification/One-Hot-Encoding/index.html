<h1 id="multi-classification">Multi Classification</h1>
<h2 id="변이-유전자-탐색-및-분류">변이 유전자 탐색 및 분류</h2>

<p>머신러닝과 딥러닝 이론을 어느정도 공부하고 진짜 문제와 이론을 접목시켜보기 위해 캐글 같은 프로젝트에 도전해보고 있다.</p>

<p>현재는 캐글에서 <a href="https://www.kaggle.com/c/msk-redefining-cancer-treatment/data">변이 유전자 탐색</a> 프로젝트에 도전하고 있는데, 결국 1~9까지의 Mulit classification 유형이다.</p>

<p>문제는 feature가 Gene, Variation(변이), Text로 전부 categorical이라는 것. Text feature의 경우 길이도 엄청 길어서, NPL에 대한 이해도 필요할 것 같다.</p>

<p>그건 나중에 보기로 하고 우선 Gene과 Variation을 처리하자면, Gene의 nunique는 264, Variation은 2996으로 One-hot-Encoding으로 처리하면 컬럼이 너무 많아진다.</p>

<h2 id="evaluation-metrics">Evaluation Metrics</h2>

<p>또 한 가지 주의점은 Evaluation Metrics가 <a href="http://wiki.fast.ai/index.php/Log_Loss">Log Loss</a>란 것. 이진 분류인 경우 수직은 다음과 같은데</p>

\[-(ylog(p)+(1-y)log(1-p))\]

<p>class 뿐 아니라 확률까지 고려하는 방식이다. 클래스를 정확히 분류했더라도, 다른(틀린) 클래스에도 확률을 부여했다면(그래서 정답 확률이 낮아졌다면) 거기에 패널티를 부여하는 방식으로 작동한다. 범위는 0~무한대이고 당연히 낮을수록 좋은데, good bad 기준이 명확하지 않다. 따라서 랜덤모델(worst case)을 만들고 거기에서 시작하는 게 좋은 방법.</p>

<p>Metric이 Log loss라 모델이 도출하는 건 각 클래스의 확률이어야 한다.</p>

<h2 id="handling-categorical-features">Handling categorical features</h2>
<h3 id="one-hot-encoding">One Hot Encoding</h3>

<p>범주형 데이터(특히 문자형)는 보통 One-Hot-Encoding을 하는데 이것도 여러 방식이 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1) pandas
</span><span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Gene'</span><span class="p">])</span>

<span class="c1"># 2) OneHotEncoder
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">enc</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Gene'</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># 3) CountVectorizer
</span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">enc</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Gene'</span><span class="p">])</span>
</code></pre></div></div>

<p>Pandas를 이용한 1번이 가장 편하긴한데, 아래와 같이 DataFrame을 생성해 메모리 소모가 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="n">ABL1</span>	<span class="n">ACVR1</span>	<span class="n">AGO2</span>	<span class="n">AKT1</span>	<span class="n">AKT2</span>	<span class="n">AKT3</span>	<span class="n">ALK</span>	<span class="n">APC</span>	<span class="n">AR</span>	<span class="n">ARAF</span>	<span class="p">...</span>	<span class="n">TSC1</span>	<span class="n">TSC2</span>	<span class="n">U2AF1</span>	<span class="n">VEGFA</span>	<span class="n">VHL</span>	<span class="n">WHSC1</span>	<span class="n">WHSC1L1</span>	<span class="n">XPO1</span>	<span class="n">XRCC2</span>	<span class="n">YAP1</span>
<span class="mi">1019</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="p">...</span>	<span class="mi">0</span>	<span class="mi">1</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>
<span class="mi">676</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="p">...</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>
<span class="mi">243</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="p">...</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>
<span class="mi">901</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="p">...</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>	<span class="mi">0</span>
</code></pre></div></div>

<p>반면, 2),3)은 generator(맞나?)로 필요할 때 사용하기 때문에 메모리 사용은 덜하다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;</span><span class="mi">2124</span><span class="n">x235</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s">'&lt;class '</span><span class="n">numpy</span><span class="p">.</span><span class="n">int64</span><span class="s">'&gt;'</span>
	<span class="k">with</span> <span class="mi">2124</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="nb">format</span><span class="o">&gt;</span>
</code></pre></div></div>

<p>이 프로젝트를 공부하면서 3번 방법을 처음 알게 되었는데, array화시키고, reshape도 신경써줘야하는 2번에 비해 편하다.</p>

<h2 id="response-coding">Response Coding</h2>

<p>One hot encoding의 단점은 category에 nunique가 많을 경우 생성되는 컬럼이 너무 많다는 것. 해당 프로젝트의 Metric이 각 클래스의 확률을 고려하는 Log Loss란 점을 고려해 Response Coding을 이용하는 게 좋아보이는데 Response Coding에 대해선 다음 글에 다루겠다.</p>

